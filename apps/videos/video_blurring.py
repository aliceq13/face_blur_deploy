# -*- coding: utf-8 -*-
"""
ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ ëª¨ë“ˆ (Optimized with Saved Tracking Data)

ì´ ëª¨ë“ˆì€ ì›ë³¸ ë¹„ë””ì˜¤ì™€ ì–¼êµ´ ì •ë³´ë¥¼ ë°›ì•„, ì§€ì •ëœ ì–¼êµ´ì„ ë¸”ëŸ¬ ì²˜ë¦¬í•œ ìƒˆë¡œìš´ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

í•µì‹¬ ìµœì í™”:
1. Saved Frame-level BBox: ë¶„ì„ ë‹¨ê³„ì—ì„œ ì €ì¥í•œ bbox ì‚¬ìš©
2. Instance-based Blur: instance_id ê¸°ë°˜ ë¸”ëŸ¬ ê²°ì •
3. Efficient Single-Pass: í•œ ë²ˆì˜ ìˆœíšŒë¡œ ì²˜ë¦¬ ì™„ë£Œ
4. ì ì‘í˜• ë¸”ëŸ¬: ì–¼êµ´ í¬ê¸°ì— ë”°ë¥¸ ë™ì  ë¸”ëŸ¬ ê°•ë„
"""

import cv2
import numpy as np
import torch
from typing import List, Dict, Optional
import logging
import os
import subprocess
import shutil
import gc

logger = logging.getLogger(__name__)


class VideoBlurrer:
    """
    ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ í´ë˜ìŠ¤ (Optimized)

    ë§¤ í”„ë ˆì„ë§ˆë‹¤:
    1. ì €ì¥ëœ bbox ë°ì´í„° ì¡°íšŒ (frame_dataì—ì„œ)
    2. instance_id ê¸°ë°˜ ë¸”ëŸ¬ ì—¬ë¶€ íŒë‹¨ (is_blurred í•„ë“œ)
    3. is_blurred=Trueì´ë©´ ë¸”ëŸ¬ ì ìš©, Falseì´ë©´ ë³´ì¡´
    4. í”„ë ˆì„ ì €ì¥
    """

    def __init__(
        self,
        yolo_model_path: str,
        device: str = 'auto',
        threshold: float = 0.92,
        use_multi_embedding: bool = False
    ):
        """
        VideoBlurrer ì´ˆê¸°í™”

        Note: í˜„ì¬ ìµœì í™”ëœ ë°©ì‹ì—ì„œëŠ” ì €ì¥ëœ bboxë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
        YOLO/AdaFace ëª¨ë¸ì€ ë¡œë“œë˜ì§€ë§Œ process_video()ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.
        í–¥í›„ ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°ë¥¼ ìœ„í•´ ìœ ì§€.
        """
        self.threshold = threshold
        self.use_multi_embedding = use_multi_embedding

        if device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        logger.info(f"Initializing VideoBlurrer on {self.device}")

        # YOLO Face ëª¨ë¸ ë¡œë“œ (í–¥í›„ ì‹¤ì‹œê°„ ì²˜ë¦¬ìš©)
        from ultralytics import YOLO
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(self.device)
        logger.info(f"YOLO Face model loaded: {yolo_model_path}")

    def _apply_blur(
        self,
        frame: np.ndarray,
        x1: int, y1: int, x2: int, y2: int,
        blur_type: str = 'pixelate',
        blur_strength: int = 15,
        padding: int = 20
    ) -> np.ndarray:
        """
        ì–¼êµ´ ì˜ì—­ì— ë¸”ëŸ¬ ì ìš©

        Args:
            frame: ì›ë³¸ í”„ë ˆì„
            x1, y1, x2, y2: ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤
            blur_type: 'pixelate' ë˜ëŠ” 'gaussian'
            blur_strength: ë¸”ëŸ¬ ê°•ë„
            padding: ì–¼êµ´ ì£¼ë³€ íŒ¨ë”©

        Returns:
            ë¸”ëŸ¬ ì²˜ë¦¬ëœ í”„ë ˆì„
        """
        h, w = frame.shape[:2]

        # íŒ¨ë”© ì¶”ê°€
        x1_pad = max(0, x1 - padding)
        y1_pad = max(0, y1 - padding)
        x2_pad = min(w, x2 + padding)
        y2_pad = min(h, y2 + padding)

        roi = frame[y1_pad:y2_pad, x1_pad:x2_pad]

        if roi.size == 0:
            return frame

        if blur_type == 'pixelate':
            # í”½ì…€í™” (ëª¨ìì´í¬)
            roi_h, roi_w = roi.shape[:2]
            factor = max(1, max(roi_h, roi_w) // blur_strength)

            if factor > 1:
                small = cv2.resize(roi, (roi_w // factor, roi_h // factor), interpolation=cv2.INTER_NEAREST)
                blurred_roi = cv2.resize(small, (roi_w, roi_h), interpolation=cv2.INTER_NEAREST)
            else:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ fallback
                k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
                k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
                blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        elif blur_type == 'gaussian':
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
            k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
            blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        else:
            # ê¸°ë³¸ê°’: ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
            k_w = max((x2_pad - x1_pad) // 3 | 1, 3)
            k_h = max((y2_pad - y1_pad) // 3 | 1, 3)
            blurred_roi = cv2.GaussianBlur(roi, (k_w, k_h), 30)

        frame[y1_pad:y2_pad, x1_pad:x2_pad] = blurred_roi
        return frame

    def process_video(
        self,
        video_path: str,
        output_path: str,
        face_models: List[Dict],
        progress_callback: Optional[callable] = None,
        blur_type: str = 'pixelate',
        blur_strength: int = 15,
        threshold: float = 0.6
    ) -> bool:
        """
        ë¹„ë””ì˜¤ ë¸”ëŸ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Optimized with Saved Tracking Data)

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            face_models: Face ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ [{'id', 'instance_id', 'frame_data', 'is_blurred'}, ...]
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            blur_type: 'pixelate' ë˜ëŠ” 'gaussian'
            blur_strength: ë¸”ëŸ¬ ê°•ë„ (ë†’ì„ìˆ˜ë¡ ì•½í•¨)
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.6)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("=" * 80)
            logger.info("ğŸ¬ Starting Video Blur Processing (Single-Pass)")
            logger.info("=" * 80)

            # threshold ì—…ë°ì´íŠ¸
            self.threshold = threshold

            # 1. í”„ë ˆì„ë³„ bbox ë°ì´í„° êµ¬ì¡°í™”
            # frame_faces[frame_idx] = [(instance_id, bbox, is_blurred), ...]
            frame_faces = {}

            logger.info(f"ğŸ“‹ Total face_models: {len(face_models)}")
            logger.info("ğŸ”„ Building frame-level bbox index...")

            for fm in face_models:
                is_blurred_val = fm.get('is_blurred', True)
                instance_id = fm.get('instance_id')
                frame_data = fm.get('frame_data', {})

                logger.info(f"Face instance {instance_id}: is_blurred={is_blurred_val}, frames={len(frame_data)}")

                # frame_data êµ¬ì¡°: {frame_idx: [x1, y1, x2, y2, conf], ...}
                for frame_idx_str, bbox_with_conf in frame_data.items():
                    frame_idx = int(frame_idx_str)

                    if frame_idx not in frame_faces:
                        frame_faces[frame_idx] = []

                    # bboxëŠ” [x1, y1, x2, y2, conf] í˜•íƒœ
                    x1, y1, x2, y2 = map(int, bbox_with_conf[:4])

                    frame_faces[frame_idx].append({
                        'instance_id': instance_id,
                        'bbox': (x1, y1, x2, y2),
                        'is_blurred': is_blurred_val
                    })

            logger.info(f"âœ… Indexed {len(frame_faces)} frames with face data")

            # í†µê³„
            total_indexed_faces = sum(len(faces) for faces in frame_faces.values())
            preserved_instances = sum(1 for fm in face_models if not fm.get('is_blurred', True))
            logger.info(f"ğŸ“Š Total indexed faces: {total_indexed_faces}")
            logger.info(f"ğŸ¯ Instances to preserve (not blur): {preserved_instances}")

            # 2. ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0

            logger.info(f"ğŸ“¹ Video: {width}x{height}, {fps} fps, {total_frames} frames")

            # 3. VideoWriter ìƒì„±
            temp_output = output_path + ".temp.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))

            if not out.isOpened():
                raise RuntimeError(f"Failed to open VideoWriter: {temp_output}")

            # 4. í”„ë ˆì„ë³„ ì²˜ë¦¬ (ìµœì í™”: ì €ì¥ëœ bbox ì‚¬ìš©, YOLO/AdaFace ì¬ì‹¤í–‰ ë¶ˆí•„ìš”)
            frame_idx = 0
            blur_count = 0
            preserved_count = 0

            logger.info("ğŸï¸  Processing frames with saved tracking data...")

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # ì €ì¥ëœ bbox ë°ì´í„° ì‚¬ìš© (YOLO ì¬ì‹¤í–‰ ì—†ìŒ!)
                if frame_idx in frame_faces:
                    for face_info in frame_faces[frame_idx]:
                        x1, y1, x2, y2 = face_info['bbox']
                        is_blurred = face_info['is_blurred']

                        # ì¢Œí‘œ ë³´ì •
                        x1, y1 = max(0, x1), max(0, y1)
                        x2, y2 = min(width, x2), min(height, y2)

                        # ë„ˆë¬´ ì‘ì€ ì–¼êµ´ì€ ìŠ¤í‚µ
                        if (x2 - x1) < 20 or (y2 - y1) < 20:
                            continue

                        # â­ instance_id ê¸°ë°˜ ë¸”ëŸ¬ ê²°ì • (AdaFace ì¬ì‹¤í–‰ ì—†ìŒ!)
                        if is_blurred:
                            # is_blurred=True â†’ ë¸”ëŸ¬ ì²˜ë¦¬
                            frame = self._apply_blur(
                                frame, x1, y1, x2, y2,
                                blur_type=blur_type,
                                blur_strength=blur_strength
                            )
                            blur_count += 1
                        else:
                            # is_blurred=False â†’ ë³´ì¡´ (ë¸”ëŸ¬ ì—†ìŒ)
                            preserved_count += 1

                # í”„ë ˆì„ ì €ì¥
                out.write(frame)

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                frame_idx += 1
                if progress_callback and frame_idx % 30 == 0:
                    pct = int((frame_idx / total_frames) * 90)
                    pct = min(pct, 90)
                    progress_callback(pct)

                    if frame_idx % 300 == 0:
                        logger.info(
                            f"ğŸ“Š Processed {frame_idx}/{total_frames} frames | "
                            f"Blurred: {blur_count} | Preserved: {preserved_count}"
                        )

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if frame_idx % 500 == 0:
                    gc.collect()

            cap.release()
            out.release()

            logger.info(f"âœ… Processing completed: {frame_idx} frames")
            logger.info(f"ğŸ“Š Blurred: {blur_count}, Preserved: {preserved_count}")

            # 5. H.264 ì¸ì½”ë”©
            logger.info("ğŸï¸  Encoding to H.264...")
            self._encode_h264(temp_output, output_path)

            if progress_callback:
                progress_callback(100)

            logger.info("=" * 80)
            logger.info("âœ… Video processing completed successfully!")
            logger.info("=" * 80)

            return True

        except Exception as e:
            logger.error(f"âŒ Video processing failed: {e}", exc_info=True)
            return False

    def _encode_h264(self, input_path: str, output_path: str):
        """FFmpegë¡œ H.264 ì¸ì½”ë”©"""
        try:
            cmd = [
                'ffmpeg', '-y',
                '-i', input_path,
                '-c:v', 'libx264',
                '-preset', 'fast',
                '-crf', '23',
                '-c:a', 'aac',
                '-movflags', '+faststart',
                output_path
            ]

            logger.info(f"ğŸ¬ Running FFmpeg: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(input_path):
                os.remove(input_path)

            logger.info("âœ… FFmpeg encoding completed")

        except subprocess.TimeoutExpired as e:
            logger.error("âŒ FFmpeg encoding timeout")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg timeout)")

        except subprocess.CalledProcessError as e:
            # FFmpeg ì—ëŸ¬ ì‹œ stderr ë¡œê·¸ ì¶œë ¥
            stderr_output = e.stderr.decode('utf-8', errors='ignore') if e.stderr else 'No stderr'
            logger.error(f"âŒ FFmpeg encoding failed (returncode={e.returncode}): {stderr_output[:500]}")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg failed)")

        except Exception as e:
            logger.error(f"âŒ FFmpeg encoding failed: {e}")
            # Fallback: ì„ì‹œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if os.path.exists(input_path) and not os.path.exists(output_path):
                shutil.move(input_path, output_path)
                logger.warning("âš ï¸  Using temp file as output (FFmpeg failed)")
